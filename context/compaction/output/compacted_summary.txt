Below is a single, consolidated summary of everything you provided/collected so far. I organized it by ADR / feature area and included the key decisions, rationale, CLI flags, implementation notes, tests, examples, docs, and recommended next steps — preserving all substantive details you recorded.

If you want this exported into separate files (PR body, unified diffs, workflow YAML, example scripts, docs), tell me which items and I’ll generate them.

---

HIGH-LEVEL CONTEXT
- Problem: Fragmented Python tooling and the need for a unified, high-performance, developer-friendly toolchain and LLM integration for CLLM.
- Overarching goals: Performance, unified UX, reproducibility (lockfiles/workspaces), fewer tools to learn, ecosystem compatibility.
- Primary technology choices:
  - Use uv (Astral's Rust package manager) as the package manager (ADR-0001).
  - Use LiteLLM Python SDK for multi-provider LLM abstraction (ADR-0002).
  - Implement local-first, bash-oriented CLI tooling with features for configuration, context injection, conversation management, and structured output.

---

ADR-0001 — Use uv as Python Package Manager
- Decision: Adopt uv (Rust-based) as unified Python package & environment manager.
- Rationale: 10–100x faster, unifies pip/pip-tools/poetry/pipx/pyenv/virtualenv, provides lockfiles, workspaces, Python version management.
- Consequences: Consolidation benefits, must learn new tool, tool currently 0.x but production-ready.
- Operational Guidance (AI agents):
  - Use uv commands (uv add, uv sync, uv run) not pip in scripts/docs/CI.
  - CI: install uv (curl installer) and use uv add / uv run in workflows.
- Where referenced: docs/decisions/0001-use-uv-as-package-manager.md

---

ADR-0002 — Use LiteLLM for LLM Provider Abstraction
- Decision: Use LiteLLM Python SDK to provide OpenAI-compatible interface for 100+ providers.
- Rationale: Single API (completion/acompletion/stream), streaming, async, function calling, provider mapping, maintained by BerriAI.
- Implementation summary (completed):
  - Core library: src/cllm/client.py (LLMClient) with complete(), acomplete(), chat(), streaming support, raw responses, api key management.
  - CLI: src/cllm/cli.py supporting prompt input, model selection, streaming, temperature, max tokens, raw JSON output.
  - pyproject: litellm>=1.79.0 dependency; project scripts and build config updated.
  - Examples: examples/basic_usage.py, async_usage.py, provider_comparison.py.
  - Tests: tests/test_client.py (11 unit tests passing).
  - Packaging: src/ layout and [tool.hatch.build.targets.wheel] configured.
- Notes:
  - LLMClient wrapper simplifies LiteLLM (string→messages conversion, default text extraction, raw response opt-out).
  - CLI entrypoint: cllm = "cllm.cli:main".

---

Implementation Notes for ADR-0002 (detailed)
- Achieved multi-provider compatibility, streaming support, sync/async APIs, and CLI integration.
- Test strategy: mock litellm.completion to avoid real API calls; tests focus on wrapper contract.
- Examples demonstrate provider switching by model name only.
- Follow-ups: provider-specific optimizations, config file support (.cllmrc), REPL, caching, cost tracking.

---

ADR-0003 — Cllmfile Configuration System (YAML + cascading lookup)
- Decision: Use YAML files (Cllmfile.yml) with cascading lookup:
  1. CLI flags
  2. Named config (--config <name> -> <name>.Cllmfile.yml)
  3. ./Cllmfile.yml
  4. ./.clm/Cllmfile.yml
  5. ~/.cllm/Cllmfile.yml
- Schema supports all LiteLLM options (model, temperature, max_tokens, stream, response_format, metadata, logging, fallbacks, context_window_fallbacks, default_system_message, raw_response).
- Features:
  - Env-var interpolation (${VAR_NAME})
  - Precedence rules: CLI > named config > file discovery
  - --show-config to print merged config
  - PyYAML dependency added (uv add pyyaml)
- Implementation status: Completed with tests (27 config tests added in one implementation cycle), examples, docs, and --show-config flag.

---

ADR-0005 — JSON Structured Output (JSON Schema)
- Decision: Hybrid approach — allow JSON Schema supplied via:
  - CLI inline (--json-schema '<json>'),
  - CLI file (--json-schema-file ./schema.json),
  - Cllmfile inline json_schema:
  - Cllmfile json_schema_file: path (CWD/.cllm lookup).
- Precedence: CLI inline > CLI file > Cllmfile inline > Cllmfile file.
- Integration:
  - Render schema to LiteLLM response_format param:
    response_format={"type":"json_schema","json_schema": {"name":"response_schema","schema": {...}}}
  - Schema validation done post-response using jsonschema.
  - Option: --validate-schema to test schema without LLM call.
- Implementation status:
  - Completed (Phase 1-3): flags, config support, path resolution, jsonschema usage, --validate-schema flag, example schemas, docs.
  - Tests: 27 new structured output tests; total full test count mentioned = 74 at that stage; all passing.
- Streaming + validation: For streaming responses, collect chunks and validate assembled response at end (latency tradeoff).
- Future enhancements: retry-on-invalid-output, streaming validation improvements.

---

LiteLLM Streaming Support
- Problem: LiteLLM v1.79.0 returns async generators for stream=True; prior sync iterator patterns broke.
- Decision: Hybrid approach — iterate streaming chunks for real-time CLI display, collect chunks in memory, then reconstruct a full response via litellm.stream_chunk_builder().
- Implementation pattern:
  - Use acomplete() and asyncio.run() wrapper for sync API to handle async generator.
  - Client._async_stream_and_build collects chunks, prints deltas in real time (content from chunk.choices[0].delta.content), then returns stream_chunk_builder(chunks, messages).
  - CLI no longer iterates returned object; client handles printing and returns final response (string/dict).
- Tests: Added streaming unit tests mocking async generator; manual verification with real providers (gpt-3.5-turbo).
- Status: Implemented; all streaming tests passing; 132 tests passing in that context.

---

ADR-0011 — Dynamic Context Injection via Command Execution (hybrid config + CLI)
- Decision: Hybrid approach — context_commands defined in Cllmfile.yml + CLI ad-hoc commands via --exec (multiple).
- Cllmfile schema supports array of context_commands with fields: name, command, on_failure (warn|ignore|fail), timeout, shell flag (dangerous opt-in), max_output.
- Execution model:
  - Commands run before LLM call.
  - Commands run in parallel with asyncio concurrency (default 4).
  - Output captured (stdout + stderr), truncated if >64 KiB default, formatted into labeled blocks and injected as context blocks in system message or prompt prefix.
  - Safety: shell=False by default, explicit shell:true require confirmation (CLI flag or env var CLLM_CONFIRM_CONTEXT_EXEC=1).
  - Error handling per on_failure.
- Implementation details:
  - New module src/cllm/context.py with ContextCommand, CommandResult, async execution helpers, format_command_block()
  - CLI flags: --exec, --no-context-exec, --confirm-context-exec, --context-concurrency
  - Tests: unit tests for parsing and execution, mock-based for timeouts/errors.
- Examples: examples/bash scripts include using context commands.
- Next steps: .cllmrc parsing, more provider-specific helpers.

---

Jinja2 Template Support for Context Commands (ADR-0012)
- Decision: Use Jinja2 SandboxedEnvironment + StrictUndefined for variable templating in context commands.
- Variable sources & precedence:
  - 1. CLI --var KEY=VALUE (highest)
  - 2. Environment variables
  - 3. Cllmfile variables: default values and required variables (None indicates required)
- Features:
  - Create Jinja2 env with sandbox and shellquote filter (shlex.quote).
  - Render templates with explicit filters and conditionals, e.g., cat {{ FILE | shellquote }} {% if VERBOSE %}-n{% endif %}
  - Validation: raise helpful errors (TemplateSyntaxError, UndefinedError).
  - Security: sandbox prevents file access, attribute access; require explicit shellquote for safe shell injection.
  - Tests: unit tests for precedence, rendering, filters, and security.
- CLI flags: --var KEY=VALUE (multiple).
- Implementation status: Implemented with tests and example scripts; templates cached and errors surfaced.

---

ADR-0013 — LLM-Driven Dynamic Command Execution (Tool/Function Calling)
- Decision: Use LiteLLM tool/function calling to allow LLM to request commands during reasoning (agentic loop).
- Key points:
  - Tool definition generation from config: available_commands list with descriptions improves LLM decision-making.
  - Safety: allowlist/denylist patterns; deny list wins; require explicit opt-in using --allow-commands; optional require_confirmation.
  - Agent loop:
    - Send prompt + tool definition to model.
    - If model emits tool call, validate command via is_command_allowed(), optionally confirm user, execute with secure subprocess wrapper, append tool output to messages, and continue.
    - Stop after max_commands (configurable).
  - CLI flags: --allow-commands, --command-allow PATTERN, --command-deny PATTERN, --confirm-commands, --max-dynamic-commands N, --dynamic-timeout.
  - Fallback: if provider/model lacks tool-calling support, show an error recommending compatible models.
- Implementation modules planned: src/cllm/tools.py (tool gen + allow/deny), src/cllm/agent.py (loop), reuse src/cllm/context.py for execution.
- Tests: mock tool calling behavior; safety tests; integration mock tests.
- Examples and docs: specific available_commands with descriptions recommended to guide LLM.

---

ADR-0014 — JSON Structured Output with --allow-commands
- Decision: Allow combining --allow-commands (dynamic commands) with --json-schema (structured output).
- Execution order: commands run (agentic loop) → LLM final inference → validate final output against JSON Schema.
- Validation:
  - Use jsonschema to validate final LLM output.
  - On failure: return clear validation errors, non-zero exit, raw response saved for inspection.
- Implementation status: Implemented; response_format passed to litellm.completion in the agent loop; tests added (7 new integration tests plus unit tests); total tests mentioned passed (213 in that stage).
- Notes: This enables programmatic processing of results after LLM-driven dynamic data gathering.

---

ADR-0015 — `cllm init` to bootstrap .cllm directories and templates
- Decision: Add cllm init subcommand with flags:
  - --local (default), --global, --template <name>, --list-templates, --force.
- Behavior:
  - Creates .cllm/ (or ~/.cllm/) with conversations/ and default Cllmfile.yml (or named template file <template>.Cllmfile.yml).
  - Adds .cllm/conversations/ to .gitignore for local init (idempotent).
  - Template discovery uses importlib.resources with dev fallback to examples/configs/.
  - Output prints checkmarks and next steps.
- Templates included: code-review, summarize, creative, debug, extraction, task-parser, context-demo.
- Tests: 26 unit tests added; full test suite passing (239 at that stage).
- Implementation refinements:
  - Templates now saved as <template>.Cllmfile.yml (not overwrite default Cllmfile.yml).
  - Force flag semantics clarified; idempotency enforced.

---

ADR-0016 — Configurable `.cllm` Directory Path (CLI + Env var)
- Decision: Support both --cllm-path and CLLM_PATH environment variable; precedence:
  1. CLI flag --cllm-path
  2. Env var CLLM_PATH
  3. Local ./.clm/
  4. Global ~/.cllm/
  5. Current directory fallback
- Path resolution: relative -> cwd, validate existence, use pathlib.Path, show effective source in --show-config.
- Affects config/resolution, conversation storage, init.
- Tests: 13 new tests; full suite passing (252 at that stage).
- Notes: Implemented get_cllm_base_path(), validated in CLI before initializing components.

---

ADR-0017 — Configurable Conversations Path (CLI + Env + config)
- Decision: Allow independent override of conversation storage path via:
  - CLI: --conversations-path (highest precedence)
  - Env: CLLM_CONVERSATIONS_PATH
  - Cllmfile.yml: conversations_path key
  - Fallbacks: custom .clm path / local / global
- Precedence: CLI > env > Cllmfile > cllm-path/defaults.
- Behavior:
  - Relative paths resolved from cwd; create directory if parent exists; symlinks followed.
  - Works orthogonally with --cllm-path (config still from cllm path, conversations from separate path).
- Tests: 19 new tests; full suite passing (271 at that stage).
- Docs: examples for shared storage, Docker, CI.

---

Read-Only Conversation Flag
- Decision: Add --read-only flag usable with --conversation to use conversation as context but prevent saving new messages.
- Validation: --read-only requires --conversation and conversation must exist.
- CLI-layer implementation: skip manager.save() calls when read-only set; no core ConversationManager API change.
- Tests: 6 new tests; full suite passing (277 at that stage).
- Use cases: templates, A/B testing, report generation, shared contexts.
- Future: add readonly in Cllmfile, conversation metadata to mark immutable, fork-on-readonly flag.

---

Stream & Agent Implementation Notes
- Streaming implemented via asyncio wrapper around LiteLLM acompletion(); prints chunks in real-time, collects to reconstruct final response via stream_chunk_builder().
- Agentic loop uses tool-calling support (LiteLLM) and validates/executes commands; uses is_command_allowed and deny/allow lists; messages appended as tool messages in conversation history (ADR-0007).
- Errors: providers without tool-calling produce user-facing error/guide to switch models.
- Important CLI flags across features:
  - Dependency and guard flags: --config, --model, --stream, --json-schema, --json-schema-file, --validate-schema, --allow-commands, --command-allow, --command-deny, --confirm-commands, --exec, --no-context-exec, --context-concurrency, --var (Jinja2), --read-only, --cllm-path, --conversations-path, --list-models, --list-configs, --show-config, --init related flags.

---

Examples, Templates, and Scripts
- Example files created:
  - examples/basic_usage.py, async_usage.py, provider_comparison.py
  - examples/configs/ templates: code-review.Cllmfile.yml, summarize.Cllmfile.yml, creative.Cllmfile.yml, extraction, task-parser, context-demo
  - examples/bash scripts: prompt-loop.sh, git-diff-review.sh, cron-digest.sh, mock cllm shim, smoke_test.sh
  - example for combined dynamic commands + JSON schema suggested: examples/dynamic_commands_with_schema.sh (recommended)
- Templates are discoverable via cllm init --list-templates and copied as <name>.Cllmfile.yml.

---

Testing Summary & Counts (as reported across implementations)
- ADR-0002: 11 client tests (all passing)
- Later combined test counts and statuses reported at various stages:
  - 38 tests (client + config) passing in a stage
  - 47 tests total at another stage (38 + 9)
  - 74 tests (struct output + others)
  - 132 total (after streaming implementation)
  - 239 tests (init implemented)
  - 252 tests (cllm path)
  - 271 tests (conversations path)
  - 277 tests (read-only flag)
- Across the full project these counts evolved as features were added; at latest stage full test suite reported passing (various counts depending on stage; last reported: 277 tests passing with read-only implemented).

---

CI / Release: GitHub Actions (decided)
- Chosen platform: GitHub Actions with two main workflows:
  1. ci.yml (PR checks): lint (ruff), mypy, pytest across Python 3.8–3.12, build verification; prefer uv-native dev installation if lockfile is present; caching for pip.
  2. release.yml (on tag push v*.*.*): re-run checks, build wheel/sdist, publish to PyPI (pypa/gh-action-pypi-publish), create GitHub Release; optionally stage to TestPyPI first.
- Recommended secrets: PYPI_API_TOKEN (and optional TEST_PYPI_TOKEN), provider API keys for integration tests stored as secrets if used.
- Targets: fast PR feedback (<5m), robust releases, type checking enforcement, coverage gating optional.
- Additional suggestions: split quick/slow CI jobs, nightly integration tests, Dependabot, changelog automation.

---

Security and Safety Practices (applies across features)
- Explicit opt-ins for high-risk features:
  - --allow-commands for agentic execution
  - shell: true in context commands only with explicit confirmation (CLLM_CONFIRM_CONTEXT_EXEC or --confirm-context-exec)
  - --read-only prevents accidental writes
- Template rendering uses Jinja2 SandboxedEnvironment + StrictUndefined and shellquote filter to avoid code injection and require explicit quoting when necessary.
- Command validation:
  - denylist prioritized over allowlist
  - available_commands with descriptions recommended for tool calling
  - require_confirmation or interactive prompts optional
- Logging & debug controls:
  - --debug, --json-logs, --log-file flags implemented (li tell: set litellm.set_verbose, litellm.json_logs)
  - Messages/warnings documented about API keys and sensitive data exposure when debug enabled.
- Path validation:
  - --cllm-path and --conversations-path validate directories exist (fail fast), and auto-create subdirs where appropriate; warn on ephemeral dirs like /tmp.

---

Observability / Debugging & UX
- Debug flags: --debug, --json-logs, --log-file (0600 file creation when possible), env var equivalents (CLLM_DEBUG, CLLM_JSON_LOGS, CLLM_LOG_FILE) and Cllmfile support.
- --show-config prints merged config with source annotations for each field (CLI/env/file/default).
- Logging and warnings advise about API key exposure in debug mode.
- For streaming and tool-calling, CLI ensures streaming print behavior and avoids duplicate printing when client reconstructs final responses.

---

Performance & Costs
- uv recommended for dev speed and reproducibility.
- Streaming uses in-memory chunk collection; memory overhead expected low but monitoring/benchmarking recommended for very large responses.
- Agentic loops and tool-calls increase token usage — monitor token/costs.
- CI design: caching, uv lockfile usage, and split quick/slow jobs recommended to keep PR feedback fast and costs low.

---

Files / Modules Created or Modified (core)
- src/cllm/client.py — LLMClient, complete/acomplete, streaming wrapper
- src/cllm/cli.py — Main CLI, flags, help text, show-config integration, init subcommand
- src/cllm/config.py — Cllmfile loader, get_cllm_base_path, variable merge
- src/cllm/context.py — context command execution helpers (async)
- src/cllm/templates.py — Jinja2 sandbox env and rendering helpers
- src/cllm/tools.py — tool definition generator and allow/deny logic
- src/cllm/agent.py — agentic execution loop, tool call validation and execution
- src/cllm/init.py — init command logic and template discovery / copy
- src/cllm/conversation.py — ConversationManager (path/ storage) modifications to accept conversation path overrides
- tests/ — many test modules added/updated (test_client.py, test_config.py, test_templates.py, test_context.py, test_cli.py, test_init.py, test_agent.py, test_conversation.py)
- examples/ — examples/configs/ templates, examples/bash scripts, examples/schemas/ for structured output
- docs/ — ADR files & usage docs (structured-output.md, dynamic-commands.md, init.md, etc.)

---

Key CLI Flags & Config Keys (comprehensive)
/ core flags:
- --config <name> (Cllmfile named configs)
- --model <model>
- --stream
- --json-schema <json> (or --json-schema-file)
- --validate-schema
- --allow-commands
- --command-allow <pattern>
- --command-deny <pattern>
- --confirm-commands
- --exec "command" (multiple)
- --no-context-exec
- --context-concurrency N
- --var KEY=VALUE (for Jinja2 templates)
- --read-only (conversation mode)
- --conversation <id>
- --cllm-path <dir> (override .clm base)
- --conversations-path <dir> (override conversations storage)
- --list-models
- --list-configs
- --show-config
- cllm init [--global] [--local] [--template <name>] [--list-templates] [--force]

Config keys (Cllmfile.yml):
- model, temperature, max_tokens, stream, response_format, api_key, api_base, timeout, num_retries, fallbacks, context_window_fallback_dict, metadata, user, input_cost_per_token, output_cost_per_token, extra_headers, raw_response, default_system_message
- context_commands: list of {name, command, on_failure, timeout, shell, max_output}
- variables: map of variable defaults (for Jinja2 templating)
- dynamic_commands: allow/deny/available_commands config for agentic tool calling
- conversations_path (for conversation storage override)
- allow_dynamic_commands, require_confirmation, max_commands (agent config)
- debug, json_logs, log_file (for logging config)

---

Testing Recommendations & Expectations
- Unit tests should cover parsing, precedence and validation for all config paths.
- Mock-based tests for LiteLLM interactions (avoid real API keys in unit tests).
- Integration tests (mocked or limited real provider) for streaming, tool-calling, and JSON schema validation.
- Add CI matrix to run tests across supported Python versions (3.8–3.12).
- Keep PR CI fast by splitting quick checks (lint/mypy) and slower integration tests; run heavy integration tests on schedule or on labeled PRs.

---

Operational / Onboarding Recommendations
- Documentation: README quickstarts for:
  - uv install and usage
  - cllm init local/global and templates
  - basic multi-provider usage with LiteLLM
  - dynamic context examples (Cllmfile + --exec)
  - Jinja2 --var examples and safety tips (use shellquote)
  - agentic examples: use --allow-commands with allowlist and confirm flags
  - combined --allow-commands + --json-schema sample pipeline
  - conversation management: --conversation, --read-only, --conversations-path
  - CI and release: how to push tag for release and configure secrets
- Onboarding: encourage use of cllm init and example templates to reduce friction.

---

Open / Recommended Next Steps (prioritized)
P0
- Add combined example script showing dynamic command execution + JSON Schema (examples/dynamic_commands_with_schema.sh).
- Add README Docker & GitHub Actions examples demonstrating custom paths and volumes.

P1
- Add end-to-end integration tests using a staging provider account (run optional, gated by secrets) to validate provider-specific tool-calling and JSON mode behavior.
- Add optional --fork-if-readonly or --fork-conversation flag for easier experiments that start from read-only context but want to save results.

P2
- Add advanced security features: symlink control, path sandboxing, read-only metadata in conversation files, conversation permission system.
- Add logs/telemetry (opt-in) for token usage and command execution counts.

P3
- Add GUI/REPL interface for interactive conversation management, and cloud-backed conversation sync (optional).

---

If you want, I will:
- Produce a unified PR bundle for one of the feature groups (e.g., agent + tool calling + tests), OR
- Generate the CI YAML files and a TestPyPI staging step, OR
- Produce the exact code files for any particular module (e.g., src/cllm/agent.py, src/cllm/tools.py, src/cllm/templates.py), OR
- Produce docs/example scripts (init example, combined schema+commands example), OR
- Export the above as a single markdown file for stakeholder review.

Which artifact(s) should I prepare first?
